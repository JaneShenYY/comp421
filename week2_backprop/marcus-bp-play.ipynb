{
 "metadata": {
  "name": "",
  "signature": "sha256:909d1853adca1c41889d62418ec368988b53a66535eb7db8a0751823ab437f06"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "Note the use of \"checkgrad\" - a very common, useful, thing to have.\n",
      "\n",
      "It \"works\" (by some measure).\n",
      "Issues:\n",
      "  * the neural net has no biases yet\n",
      "  * my \"outer product\" has an evil loop over training patterns in it: ugly and slow!\n",
      "  * the learning problem is just random - better if we could read in a training set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 320
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# THESE FUNKS MUST MATCH ONE ANOTHER................\n",
      "\n",
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    return x\n",
      "\n",
      "\"\"\"\n",
      "def dfunk( phi ):\n",
      "    # this is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    x = funk(phi)\n",
      "    dx = x*(1-x)\n",
      "    return dx\n",
      "\"\"\"\n",
      "\n",
      "def dfunk_from_funk( x ):  # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    \n",
      "    # Again, this is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    # But the input argument isn't phi here - it's the function value itself.\n",
      "    \n",
      "    dx = x*(1-x)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 321
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 5\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.275  0.076 -0.216 -1.066]\n",
        " [ 0.591  0.842 -0.132 -0.475]\n",
        " [-1.428 -1.71  -0.878 -0.804]]\n",
        "[[0 1]\n",
        " [0 1]\n",
        " [1 0]]\n"
       ]
      }
     ],
     "prompt_number": 322
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
      "    return Good_vec.sum(), Good_vec, dGood_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 323
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "architecture = [inpats.shape[1], 3, targets.shape[1]]\n",
      "#architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are this many neurons in each layer:  [4, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 324
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] \n",
      "# X is going to be a list giving the activations of successive layers. \n",
      "# Each one is a matrix, whose columns are the neurons in that layer.\n",
      "# Each row in the matrix corresponds to a training item.\n",
      "# So all the matrices in the list X will have the same number of rows.\n",
      "\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 activations have shape  (5, 4)\n",
        "layer 1 activations have shape  (5, 3)\n",
        "layer 2 activations have shape  (5, 2)\n"
       ]
      }
     ],
     "prompt_number": 325
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set up the weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W  = [np.array(None)]\n",
      "dW = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
      "\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
      "    dW.append(0.0 * np.copy(W[L]))\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 weights have shape  ()\n",
        "layer 1 weights have shape  (3, 4)\n",
        "layer 2 weights have shape  (2, 3)\n"
       ]
      }
     ],
     "prompt_number": 326
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X, W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 327
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test outer product - this version really sucks as it has a loop over patterns. Must Try Harder.\n",
      "def inefficient_outer_product(x_lowLayer, x_highLayer):\n",
      "    npats = x_lowLayer.shape[0]\n",
      "    n_lowLayer  = x_lowLayer.shape[1]\n",
      "    n_highLayer = x_highLayer.shape[1]\n",
      "    #print ('xlow and xhi shapes: ',x_lowLayer.shape, x_highLayer.shape)\n",
      "    ans = np.zeros((npats, n_highLayer, n_lowLayer))\n",
      "    for i in range(npats):\n",
      "        #print ('shapes: ', ans[i,:,:].shape, np.multiply.outer(x_highLayer[i], x_lowLayer[i]).shape)\n",
      "        ans[i,:,:] = np.outer(x_highLayer[i,:], x_lowLayer[i,:])\n",
      "    i=0\n",
      "    #print ('highlayer activity: ', x_highLayer[i,:])\n",
      "    #print ('lowlayer activity: ', x_lowLayer[i,:])\n",
      "    #print ('outer product: ',np.outer(x_highLayer[i,:], x_lowLayer[i,:]))\n",
      "    return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 328
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    #print ('an output: ', X[-1][0])\n",
      "    #print ('a target: ', targets[0])\n",
      "    #print ('an epsilon: ', epsilon[0])\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        #print ('layer ',L)\n",
      "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
      "        dw_per_pat = inefficient_outer_product(X[L-1], psi)\n",
      "        #print ('EG:\\t in_act=%.3f,\\n\\t out_act=%.3f, \\n\\t epsilon=%.3f,' %(X[L-1][0,0], X[L][0,0], epsilon[0]))\n",
      "        #print ('\\t dXdphi=%.3f, \\n\\t dW=%.3f' %(dfunk_from_funk(X[L])[0], dw_per_pat[0,0,0]))\n",
      "        dW[L] = dw_per_pat.sum(0) # outer product multiply\n",
      "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 329
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X, W)\n",
      "dW = backward_pass(X, W, dW, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 330
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(dW, X, W, targets):\n",
      "    # Check the gradient directly, via perturbations to every weight.\n",
      "    # This is completely daft in practical terms, but very useful for debugging.\n",
      "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
      "    tiny = 0.0001\n",
      "    \n",
      "    dW_test = [np.array(None)]\n",
      "    for L in range(1,len(W)):\n",
      "        dW_test.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    X = forward_pass(X,W)\n",
      "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "    \n",
      "    for L in range(1,len(X)):\n",
      "        for j in range(W[L].shape[0]): # index of destination node\n",
      "            for i in range(W[L].shape[1]): # index of origin node\n",
      "                # perturb that weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
      "                # compute and store the empirical gradient estimate\n",
      "                X = forward_pass(X,W)\n",
      "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny\n",
      "                \n",
      "                # unperturb the weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
      "                \n",
      "    # show the result?\n",
      "    for L in range(1,len(X)):\n",
      "        print ('-------------- layer %d --------------' %(L))\n",
      "        #print ('acts:', X[L])\n",
      "        print ('calculated gradients:')\n",
      "        print (dW[L])\n",
      "        print ('empirical gradients:')\n",
      "        print (dW_test[L])\n",
      "\n",
      "checkgrad(dW, X, W, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------- layer 1 --------------\n",
        "calculated gradients:\n",
        "[[-0.007 -0.005  0.     0.002]\n",
        " [ 0.001  0.002  0.    -0.003]\n",
        " [-0.     0.005  0.003 -0.009]]\n",
        "empirical gradients:\n",
        "[[-0.007 -0.005  0.     0.002]\n",
        " [ 0.001  0.002  0.    -0.003]\n",
        " [-0.     0.005  0.003 -0.009]]\n",
        "-------------- layer 2 --------------\n",
        "calculated gradients:\n",
        "[[ 0.013 -0.004 -0.017]\n",
        " [-0.013  0.006 -0.004]]\n",
        "empirical gradients:\n",
        "[[ 0.013 -0.004 -0.017]\n",
        " [-0.013  0.006 -0.004]]\n"
       ]
      }
     ],
     "prompt_number": 334
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## yay.\n",
      "The gradient seems to be right for the full MLP, so that's... progress!\n",
      "\n",
      "Let's try learning the problem then...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    times, vals = [], []\n",
      "    next_time = 0\n",
      "    \n",
      "    prev_change = [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        prev_change.append(0.0 * np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        \n",
      "        # this is just record-keeping.......\n",
      "        if step == next_time:\n",
      "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "            vals.append(good_sum)\n",
      "            times.append(step)\n",
      "            next_time = step + 10\n",
      "\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "\n",
      "    return W, times, vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 332
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.9, num_steps=1000)\n",
      "plt.plot(vals, times)\n",
      "print(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array(None, dtype=object), array([[-1.619, -0.671,  0.227, -0.752],\n",
        "       [ 1.684,  1.565,  0.098, -0.419],\n",
        "       [ 1.018,  1.642,  0.483, -1.59 ]]), array([[ 1.496, -2.178, -3.085],\n",
        "       [-3.502,  2.522,  1.131]])]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x443f318>"
       ]
      }
     ],
     "prompt_number": 333
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 301
    }
   ],
   "metadata": {}
  }
 ]
}