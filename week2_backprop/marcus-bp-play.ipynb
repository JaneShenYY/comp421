{
 "metadata": {
  "name": "",
  "signature": "sha256:c01426c1282903294d1a7a580880db48ba53a5dab12aa31747dc3055f0908058"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "Note the use of \"checkgrad\" - a very common, useful, thing to have.\n",
      "\n",
      "It \"works\" (by some measure).\n",
      "Issues:\n",
      "  * the neural net has no biases yet\n",
      "  * my \"outer product\" has an evil loop over training patterns in it: ugly and slow!\n",
      "  * the learning problem is just random - better if we could read in a training set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# THESE FUNKERS MUST MATCH ONE ANOTHER................\n",
      "\n",
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    \n",
      "    #ALT: rectified linear goes like this\n",
      "    #x = phi * (phi>0.0)\n",
      "    return x\n",
      "\n",
      "def dfunk_from_funk( x ):  # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    # This is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    # But the input argument isn't phi here - it's the function value itself.\n",
      "\n",
      "    dx = x*(1-x)\n",
      "    #ALT: rectified linear goes like this\n",
      "    #dx = 1.0*(x>0.0)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 50\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-1.289  1.141  0.776  0.234]\n",
        " [ 0.304 -0.323 -1.604 -1.033]\n",
        " [-0.924  0.49  -0.162  0.066]]\n",
        "[[0 0]\n",
        " [1 1]\n",
        " [0 0]]\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
      "    return Good_vec.sum(), Good_vec, dGood_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "architecture = [inpats.shape[1], 3, targets.shape[1]]\n",
      "#architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('There are this many neurons in each layer: ', [4, 3, 2])\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] \n",
      "# X is going to be a list giving the activations of successive layers. \n",
      "# Each one is a matrix, whose columns are the neurons in that layer.\n",
      "# Each row in the matrix corresponds to a training item.\n",
      "# So all the matrices in the list X will have the same number of rows.\n",
      "\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('layer 0 activations have shape ', (50, 4))\n",
        "('layer 1 activations have shape ', (50, 3))\n",
        "('layer 2 activations have shape ', (50, 2))\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set up the weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W  = [np.array(None)]\n",
      "dW = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
      "\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
      "    dW.append(0.0 * np.copy(W[L]))\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('layer 0 weights have shape ', ())\n",
        "('layer 1 weights have shape ', (3, 4))\n",
        "('layer 2 weights have shape ', (2, 3))\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X, W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    npats = X[0].shape[0]\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
      "        n1, n2 = X[L-1].shape[1], psi.shape[1]\n",
      "        A = np.tile(X[L-1],n2).reshape(npats,n2,n1)\n",
      "        B = np.repeat(psi,n1).reshape(npats,n2,n1)        \n",
      "        dW[L] = (A*B).sum(0) # outer product multiply\n",
      "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X, W)\n",
      "dW = backward_pass(X, W, dW, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(dW, X, W, targets):\n",
      "    # Check the gradient directly, via perturbations to every weight.\n",
      "    # This is completely daft in practical terms, but very useful for debugging.\n",
      "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
      "    tiny = 0.0001\n",
      "    \n",
      "    dW_test = [np.array(None)]\n",
      "    for L in range(1,len(W)):\n",
      "        dW_test.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    X = forward_pass(X,W)\n",
      "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "    \n",
      "    for L in range(1,len(X)):\n",
      "        for j in range(W[L].shape[0]): # index of destination node\n",
      "            for i in range(W[L].shape[1]): # index of origin node\n",
      "                # perturb that weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
      "                # compute and store the empirical gradient estimate\n",
      "                X = forward_pass(X,W)\n",
      "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny                \n",
      "                # unperturb the weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
      "                \n",
      "    # show the result?\n",
      "    for L in range(1,len(X)):\n",
      "        print ('-------------- layer %d --------------' %(L))\n",
      "        print ('calculated gradients:')\n",
      "        print (dW[L])\n",
      "        print ('empirical gradients:')\n",
      "        print (dW_test[L])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "checkgrad(dW, X, W, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------- layer 1 --------------\n",
        "calculated gradients:\n",
        "[[ 0.001  0.149 -0.002  0.127]\n",
        " [ 0.001  0.115 -0.009  0.097]\n",
        " [-0.011  0.061  0.033  0.053]]\n",
        "empirical gradients:\n",
        "[[ 0.001  0.149 -0.002  0.127]\n",
        " [ 0.001  0.115 -0.009  0.097]\n",
        " [-0.011  0.061  0.033  0.053]]\n",
        "-------------- layer 2 --------------\n",
        "calculated gradients:\n",
        "[[ 0.28  -0.043  0.218]\n",
        " [-0.248 -0.171 -0.078]]\n",
        "empirical gradients:\n",
        "[[ 0.28  -0.043  0.218]\n",
        " [-0.248 -0.171 -0.078]]\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## yay.\n",
      "The gradient seems to be right for the full MLP, so that's... progress!\n",
      "\n",
      "Let's try learning the problem then...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    times, vals = [], []\n",
      "    next_time = 0\n",
      "    \n",
      "    prev_change = [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        prev_change.append(0.0 * np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        \n",
      "        # this is just record-keeping.......\n",
      "        if step == next_time:\n",
      "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "            vals.append(good_sum)\n",
      "            times.append(step)\n",
      "            next_time = step + 10\n",
      "\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "\n",
      "    return W, times, vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.8, num_steps=1000)\n",
      "plt.plot(vals, times)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "[<matplotlib.lines.Line2D at 0xb104f44c>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgEAgEv9P9tQuSuRObgjAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0xb115c62c>"
       ]
      }
     ],
     "prompt_number": 83
    }
   ],
   "metadata": {}
  }
 ]
}