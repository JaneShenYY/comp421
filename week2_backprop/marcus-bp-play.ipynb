{
 "metadata": {
  "name": "",
  "signature": "sha256:ec57a20669528135a902d7368b7074214f7f615ffd2721ac6b80979f1ef32e0b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "It's NOT working yet - see the graph at the end, which should go UP UP UP!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 288
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    return x\n",
      "\n",
      "def dfunk( phi ):\n",
      "    # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    x = funk(phi)\n",
      "    dx = x*(1-x)\n",
      "    return dx\n",
      "\n",
      "def dfunk_from_funk( x ):\n",
      "    # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    dx = x*(1-x)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 289
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 5\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 2.159  0.608 -0.625 -2.566]\n",
        " [-0.803  1.157 -0.016  0.304]\n",
        " [ 0.401  0.608  1.021  1.317]]\n",
        "[[0 0]\n",
        " [1 1]\n",
        " [1 0]]\n"
       ]
      }
     ],
     "prompt_number": 290
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
      "    return Good_vec.sum(), Good_vec, dGood_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 291
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "architecture = [inpats.shape[1], 3, targets.shape[1]]\n",
      "#architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are this many neurons in each layer:  [4, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] \n",
      "# X is going to be a list giving the activations of successive layers. \n",
      "# Each one is a matrix, whose columns are the neurons in that layer.\n",
      "# Each row in the matrix corresponds to a training item.\n",
      "# So all the matrices in the list X will have the same number of rows.\n",
      "\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 activations have shape  (5, 4)\n",
        "layer 1 activations have shape  (5, 3)\n",
        "layer 2 activations have shape  (5, 2)\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set up the weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W  = [np.array(None)]\n",
      "dW = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
      "\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
      "    dW.append(0.0 * np.copy(W[L]))\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 weights have shape  ()\n",
        "layer 1 weights have shape  (3, 4)\n",
        "layer 2 weights have shape  (2, 3)\n"
       ]
      }
     ],
     "prompt_number": 305
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X, W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 306
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test outer product - this version really sucks as it has a loop over patterns. Must Try Harder.\n",
      "def inefficient_outer_product(x_lowLayer, x_highLayer):\n",
      "    npats = x_lowLayer.shape[0]\n",
      "    n_lowLayer  = x_lowLayer.shape[1]\n",
      "    n_highLayer = x_highLayer.shape[1]\n",
      "    #print ('xlow and xhi shapes: ',x_lowLayer.shape, x_highLayer.shape)\n",
      "    ans = np.zeros((npats, n_highLayer, n_lowLayer))\n",
      "    for i in range(npats):\n",
      "        #print ('shapes: ', ans[i,:,:].shape, np.multiply.outer(x_highLayer[i], x_lowLayer[i]).shape)\n",
      "        ans[i,:,:] = np.outer(x_highLayer[i,:], x_lowLayer[i,:])\n",
      "    i=0\n",
      "    #print ('highlayer activity: ', x_highLayer[i,:])\n",
      "    #print ('lowlayer activity: ', x_lowLayer[i,:])\n",
      "    #print ('outer product: ',np.outer(x_highLayer[i,:], x_lowLayer[i,:]))\n",
      "    return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 307
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    #print ('an output: ', X[-1][0])\n",
      "    #print ('a target: ', targets[0])\n",
      "    #print ('an epsilon: ', epsilon[0])\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        #print ('layer ',L)\n",
      "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
      "        dw_per_pat = inefficient_outer_product(X[L-1], psi)\n",
      "        #print ('EG:\\t in_act=%.3f,\\n\\t out_act=%.3f, \\n\\t epsilon=%.3f,' %(X[L-1][0,0], X[L][0,0], epsilon[0]))\n",
      "        #print ('\\t dXdphi=%.3f, \\n\\t dW=%.3f' %(dfunk_from_funk(X[L])[0], dw_per_pat[0,0,0]))\n",
      "        dW[L] = dw_per_pat.sum(0) # outer product multiply\n",
      "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 308
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X, W)\n",
      "dW = backward_pass(X, W, dW, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 309
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(dW, X, W, targets):\n",
      "    # Check the gradient directly, via perturbations to every weight.\n",
      "    # This is completely daft, but it's useful for debugging.\n",
      "    tiny = 0.0001\n",
      "    \n",
      "    dW_test = [np.array(None)]\n",
      "    for L in range(1,len(W)):\n",
      "        dW_test.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    X = forward_pass(X,W)\n",
      "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "    \n",
      "    for L in range(1,len(X)):\n",
      "        for j in range(W[L].shape[0]): # index of destination node\n",
      "            for i in range(W[L].shape[1]): # index of origin node\n",
      "                # perturb that weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
      "                # compute and store the empirical gradient estimate\n",
      "                X = forward_pass(X,W)\n",
      "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny\n",
      "                \n",
      "                # unperturb the weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
      "                \n",
      "    # show the result?\n",
      "    for L in range(1,len(X)):\n",
      "        print ('-------------- layer %d --------------' %(L))\n",
      "        #print ('acts:', X[L])\n",
      "        print ('calculated gradients:')\n",
      "        print (dW[L])\n",
      "        print ('empirical gradients:')\n",
      "        print (dW_test[L])\n",
      "\n",
      "checkgrad(dW, X, W, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------- layer 1 --------------\n",
        "calculated gradients:\n",
        "[[-0.018  0.002  0.002  0.016]\n",
        " [-0.021 -0.016 -0.024 -0.012]\n",
        " [-0.001 -0.003 -0.005 -0.005]]\n",
        "empirical gradients:\n",
        "[[-0.018  0.002  0.002  0.016]\n",
        " [-0.021 -0.016 -0.024 -0.012]\n",
        " [-0.001 -0.003 -0.005 -0.005]]\n",
        "-------------- layer 2 --------------\n",
        "calculated gradients:\n",
        "[[ 0.18   0.182  0.22 ]\n",
        " [-0.183 -0.202 -0.221]]\n",
        "empirical gradients:\n",
        "[[ 0.18   0.182  0.22 ]\n",
        " [-0.183 -0.202 -0.221]]\n"
       ]
      }
     ],
     "prompt_number": 310
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## yay.\n",
      "The gradient seems to be right for the full MLP, so that's... progress!\n",
      "\n",
      "Let's try learning the problem then...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    times, vals = [], []\n",
      "    next_time = 0\n",
      "    \n",
      "    prev_change = [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        prev_change.append(0.0 * np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        \n",
      "        # this is just record-keeping.......\n",
      "        if step == next_time:\n",
      "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "            vals.append(good_sum)\n",
      "            times.append(step)\n",
      "            next_time = step + 10\n",
      "\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "\n",
      "    return W, times, vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 311
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.9, num_steps=1000)\n",
      "plt.plot(vals, times)\n",
      "print(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array(None, dtype=object), array([[-1.251,  0.897,  0.458,  1.555],\n",
        "       [ 0.166, -0.805,  1.169,  1.39 ],\n",
        "       [ 2.374, -0.412,  2.138,  0.683]]), array([[ 2.281,  1.308, -0.866],\n",
        "       [ 1.931, -0.902, -3.628]])]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X9UVGX+B/A3iW2lpWkJyaAogsMwiChGluGgjUoIWZqa\nrZKp+d39trt2Omm7f3yjPQfEU20H1/acOqUidvxBLYaKpK6OZYqa4I8EFtxAfgzDajAlavzy+f7x\nBEoMIPODOz/er3PuUcY7dz7cU/c9z3Of57leQggBIiLyaHcpXQARESmPYUBERAwDIiJiGBARERgG\nREQEhgEREcEOYZCbmwu1Wo2goCCsW7fO4j5//OMfERQUhPDwcBQUFNj6kUREZGc2hUFrayteffVV\n5ObmorCwENu2bUNRUVGHfXJycnDx4kWUlpbio48+wu9+9zubCiYiIvuzKQxOnjyJMWPGICAgAP37\n98fChQvxxRdfdNgnOzsbiYmJAICoqCiYzWbU1tba8rFERGRnNoVBdXU1/P39239WqVSorq7ucZ+q\nqipbPpaIiOzMpjDw8vK6o/1+veLFnb6PiIj6hrctb/bz80NlZWX7z5WVlVCpVN3uU1VVBT8/v07H\nGjNmDP7zn//YUg4RkccJDAzExYsXbT+QsEFzc7MYPXq0KCsrE42NjSI8PFwUFhZ22Gfv3r0iNjZW\nCCHE8ePHRVRUlMVj2ViKW3nrrbeULsFp8FzcwnNxC8/FLfa6dtrUMvD29saGDRswc+ZMtLa2Ytmy\nZQgJCcGHH34IAFi5ciWefvpp5OTkYMyYMRgwYAA2bdpke4IREZFd2RQGABAbG4vY2NgOr61cubLD\nzxs2bLD1Y4iIyIE4A9kJ6XQ6pUtwGjwXt/Bc3MJzYX9ev/Q5Kc7Ly6vTqCMiIuqeva6dbBkQERHD\ngIiIGAZERASGARERgWFARERgGBARERgGREQEhgEREYFhQEREYBgQEREYBkREBIYBEZHLMJmAEycc\nc2ybl7AmIiLH+OEH4F//Ag4fBgwGoLYWWLAAiIqy/2dx1VIiIifR0gLk5QE5OcD+/UBJCRAdDUyb\nBsTEAOPGAf36dXyPva6dDAMiIgWZzfLiv3u3DIARI4DYWGDmTGDyZODuu7t/P8OAiMhFmUzAP/8J\nZGXJewBTpwIJCcDTTwN+fr07FsOAiMiF1NYCmZlyO3cOiIsDnn1WtgAGDrT+uAwDIiInZzbLFsC2\nbcCpU0B8PDB/PqDXA/fcY5/PYBgQETmhpiYgNxfYuhX48kt583fRItkSuO8++38ew4CIyEkIARQU\nAOnpshUQHAwsXgw8/zwwZIhjP9te107OMyAistLly7IFsGkT8NNPQGIicPw4EBiodGW9Z9MM5Lq6\nOuj1egQHB2PGjBkwm82d9qmsrERMTAxCQ0Oh1Wqxfv16Wz6SiEhRLS3A3r3A3LlAUJBsEaSlAd9/\nD7z9tmsGAWBjN9Hq1avx0EMPYfXq1Vi3bh3q6+uRmpraYR+TyQSTyYTx48ejoaEBEydOxK5duxAS\nEtKxEHYTEZETKy0FNm6UXUEjRwJLl8rZwIMGKVuXva6dNrUMsrOzkZiYCABITEzErl27Ou3j6+uL\n8ePHAwAGDhyIkJAQGI1GWz6WiKhPXLsmL/7R0cCUKUBzM3DwoOwKeuUV5YPAnmxqGTz44IOor68H\nAAghMGTIkPafLSkvL8fUqVNx4cIFDPzVwFq2DIjIGQgBnDwpWwGZmcDjjwPLlsnRQD3NBlZCn91A\n1uv1MJlMnV5PTk7uVJCXl1eXx2loaMC8efOQlpbWKQjaJCUltf9dp9NBp9P1VB4RkV3897/yZvDG\njUBjI/Dyy8D5872fEexoBoMBBoPB7se1qWWgVqthMBjg6+uLmpoaxMTEoLi4uNN+zc3NmD17NmJj\nY7Fq1SrLhbBlQER9rKVFzgnYuBE4dAh45hkZAtHRQDffbZ2KU9wzSEhIQHp6OgAgPT0dc+bM6bSP\nEALLli2DRqPpMgiIiPpSURGwejXg7w+kpMg1gSoq5P2BqVNdJwjsyaaWQV1dHebPn4+KigoEBARg\n586dGDx4MIxGI1asWIG9e/fi6NGjiI6Oxrhx49q7kdauXYtZs2Z1LIQtAyJyoPp6YMcOYPNmeeFf\nsgR46SVArVa6MttwBjIRUQ9aW4EDB2QA5ObKReFeekmuDeTtJlNuGQZERF0oLJRdPhkZgEolA2Dh\nQscvDaEELkdBRHSbK1fkukDp6UBNDfDb38o5ARqN0pW5BrYMiMhlNTbKpSG2bJHPCI6Lk+sDTZ/e\n+fGQ7ordRETkkYSQTwfbsgXYuRPQamUAzJ0LPPCA0tX1PXYTEZFHKSuTk8IyMuTQzyVLgNOn5TpB\nZDuGARE5rR9/lEtCbNki5wYsWCDD4NFHPXMugCOxm4iInEpzs3xCWEaGHA761FPyQTFPP+2cawMp\njfcMiMhtCAHk58sWwPbtwJgxMgDmz3fP4aD2xHsGROTyqqqATz+VIXDjhrwP8M03Mgyob7FlQER9\nqqEByMqSAZCfD8ybJ0Pg8cd5H8AabBkQkcu4eRM4fFgGwBdfyAfFvPIKEB8P3HOP0tURwJYBETnQ\nv/8tZwRv3QoMHSrnA7zwAuDjo3Rl7oMtAyJySm2rg6anA+XlwIsvAnv2AOPGKV0ZdYctAyKyWUvL\nrdVBv/wSmDFDLg43Y4b7rA7qrDi0lIgUV1QkA6BtddClS+XqoA8+qHRlnoPdRESkCLNZzgVoe0jM\n4sVcHdQdsGVARD26eVM+I3jTJtn/r9fLVsDMmewGUhq7iYjI4crLZQtg82bZ9bN0KbBoEfDQQwoX\nRu3YTUREDnH9upwUtnEjcPasHAqalQVERChdGTkSw4CI2p8RsGmTXCU0Kgr4n/8BEhKA3/xG6eqo\nLzAMiDyY0SgnhG3eLIeHvvQScO6cHBlEnoVhQORhfv4ZyM6WAXD8uFwb6KOPgCee4NpAnoxhQOQB\nhJCrgW7ZAnz2GTBhgmwFZGYCAwYoXR05A6vDoK6uDgsWLMClS5cQEBCAnTt3YvDgwRb3bW1tRWRk\nJFQqFXbv3m11sUTUO//+t+wG+vRTuSDckiXsBiLL7rL2jampqdDr9SgpKcH06dORmpra5b5paWnQ\naDTwYhuUyOGqq4G//Q2IjAR0OuDaNdkauHABePNNBgFZZnUYZGdnIzExEQCQmJiIXbt2WdyvqqoK\nOTk5WL58OecREDnIlSvAhx8C06YBWi3w3XfA2rVAZaUMhgkTeD+Aumd1N1FtbS18flmH1sfHB7W1\ntRb3e+211/DOO+/gp59+svajiMiCujpg1y5g5055I3jWLODVV+WzgvmMAOqtbsNAr9fDZDJ1ej05\nObnDz15eXha7gPbs2YNhw4YhIiICBoOhx2KSkpLa/67T6aDT6Xp8D5EnuXxZPhzms89kADz1lLwR\n/NlnwMCBSldHfcFgMNzR9bS3rF6OQq1Ww2AwwNfXFzU1NYiJiUFxcXGHff7yl78gIyMD3t7e+Pnn\nn/HTTz9h7ty52LJlS+dCuBwFkUXV1bIF8PnnwOnTcj2guXOBuDgGADnB2kSrV6/G0KFDsWbNGqSm\npsJsNnd7E/nIkSN49913uxxNxDAgkoQAiotlCyArC7h4UV74n3tOBsG99ypdITkTe107rb6B/Oab\nb+LAgQMIDg7GoUOH8OabbwIAjEYj4uLiLL6Ho4mILGtpAb7+GnjjDWDsWLkqaEUFkJwMmExyfsCc\nOQwCchyuWkqkkB9/BPbvB3bvBnJy5JDPhATgmWc4+ofunOLdRPbGMCB3J4ScBJaTI58JcOoUMGUK\nEB8PzJ4NjBihdIXkihgGRC7g2jXg8GFg3z4ZAq2tQGysvPhPm8alIMh2fJ4BkRMSQk74+vJLIDdX\nLgsdGSnnAOzeDYSGsvuHnBNbBkQ2qq2VzwA+cEDeA7j3XmDGDBkA06YB99+vdIXkzthNRKSQhgY5\n8udf/5IBUFEh1wDS6+XQz8BApSskT8IwIOojjY1AXp58IPzhw0BBATBxIjB9upwBPGkSHwpPymEY\nEDlIY6Mc6XP4MGAwACdPAhoNEBMju32mTAHuu0/pKokkhgGRnVy/Lr/5f/WV3E6dkhO/dDq5TZkC\ndPGoDiLFMQyIrHTlCnDsGHD0qLz4nz8PjBsHTJ0KREfLxz8OGqR0lUR3hmFAdAfaJnodOya3b76R\nD4F/7DF50X/ySSAqit0+5LoYBkQW/Pij7OY5flx2/eTlAQ88ADz+ODB5suzyCQsD+vVTulIi+2AY\nkMdrapJdPCdPysldJ0/KYZ4REfLCP3my/NY/fLjSlRI5DsOAPEprq1zW+dtv5XbqlAyC0aPl0M5H\nH5UXfq0W6N9f6WqJ+g7DgNxWczNQVATk58vt9Gng7FngkUfkhT8yUm4TJvDhLkQMA3ILDQ3AuXPA\nmTNyKygACgvlCp4REXJy18SJwPjxHN5JZAnDgFzKzZvApUvywn/unPymf/asfKSjRiMv/BER8qI/\nbhy/8RPdKYYBOa3Ll+XKnd99J/v1z58HLlyQ3+zHjZOjecLD5RYUxKUciGzBMCDF1dXJLp0LF25t\n330nl3PQauUWFiY3rRYYMkTpioncD8OA+oQQconmoiK5FRbe+vPaNdnFExp6a9Nq5VBOrtlP1DcY\nBmRXLS1AWZkcvllcfOviX1wM3HWXvOhrNEBIiNxCQwE/P170iZTGMCCr1NfL5RnatraL//ffy6Gb\nISGAWi23tr8//LDSVRNRVxgG1KW2b/m3X/TbLvzXr8sVOdu2tgv+mDHyCV1E5FoYBgSzueO3+7a/\nt33Lv/2ir1bLP9mfT+ReFA+Duro6LFiwAJcuXUJAQAB27tyJwRZmBZnNZixfvhwXLlyAl5cXNm7c\niMcee6xzIQwDi4SQq2y23bht688vLgauXr3VpXP7BT8oiN/yiTyF4mGwevVqPPTQQ1i9ejXWrVuH\n+vp6pKamdtovMTERU6dOxcsvv4yWlhZcu3YNgywsFu/pYdB20f/uu45DNYuKgHvu6Xjzti0AVCp+\nyyfydIqHgVqtxpEjR+Dj4wOTyQSdTofi4uIO+/z444+IiIjA999/33MhHhQGV6/KiVhts3HPn5ch\n8JvfdBym2TaCZ+hQpSsmImeleBg8+OCDqK+vBwAIITBkyJD2n9ucOXMGK1euhEajwdmzZzFx4kSk\npaXhPgtPEnHXMLhyRS60lp8v190pKJAtAI1GzsBtm5Gr1XLUDhH1nr2und0uBKDX62EymTq9npyc\n3KkYLwv9FS0tLcjPz8eGDRswadIkrFq1CqmpqfjrX/9q8fOSkpLa/67T6aDT6e7gV3Ae16/L5ZXb\n1tc/dUoO5ZwwQS62NmcOkJQEBAdzCQYiso7BYIDBYLD7cW3qJjIYDPD19UVNTQ1iYmI6dROZTCZM\nnjwZZWVlAICjR48iNTUVe/bs6VyIC7YM/vtf+Qzdo0fl4xQLC+U3/Kgoub7+o4/KIZt33aV0pUTk\nrvqkZdCdhIQEpKenY82aNUhPT8ecOXM67ePr6wt/f3+UlJQgODgYBw8eRGhoqE0FK+nqVeDQIeDA\nAcBgAKqq5GMUn3wS+Nvf5Br7HMVDRK7IpqGl8+fPR0VFRYehpUajEStWrMDevXsBAGfPnsXy5cvR\n1NSEwMBAbNq0yaVGE5WWArt2AXv3yr7/xx4DZswAYmLkcsvs7iEiJSl+A9nenCkMLl4EMjKAzz8H\nfvhB9vXHxwNTpwIDBihdHRHRLQwDO7t+Hdi2Ddi8Wc7kXbQImD9ftgTY509EzophYCc//AB88IHc\noqKAZcuA2Fjg7rv7vBQiol5T/Aayq7t+HUhJAf7xD+DZZ4EjR+SsXiIiT+SRYZCbC/zv/wKTJskZ\nwCqV0hURESnLo8Lgxg3glVfknIB//AOYNUvpioiInIPHhEFdHZCQAIwcKdcBsrAiBhGRx/KIcTJV\nVXJiWFSUHDLKICAi6sjtw+DSJeCJJ4ClS4H33uMwUSIiS9x6aGlLC6DTyQlja9bY9dBERE7BXtdO\nt/6evHatfEbAG28oXQkRkXNz2xvIeXnAhg3yOQLsGiIi6p5bXiavXgV++1s5fNTPT+lqiIicn1ve\nM1i1SgbCJ5/Y5XBERE6LaxN1ob4eCAyUD5N/5BE7FEZE5MR4A7kLH30kRw8xCIiI7pxbtQyamoDR\no4E9e+SDZ4iI3B1bBhZkZgJjxzIIiIh6y23CQAj5HOLXXlO6EiIi1+M2YfD110BDA/D000pXQkTk\netwmDNpaBZxgRkTUe25xA7m+Xi5NXVPDB9YTkWfhDeTbHDgAREczCIiIrOUWYZCTw3sFRES2sDoM\n6urqoNfrERwcjBkzZsBsNlvc7/3334dWq0VYWBgWLVqExsZGq4u15OZNYN8+IDbWroclIvIoVodB\namoq9Ho9SkpKMH36dKSmpnbap7q6Gn//+99x+vRpnD9/Hq2trdi+fbtNBf9afj4wdCgwapRdD0tE\n5FGsDoPs7GwkJiYCABITE7Fr1y6L+7W0tOD69evtf/rZeRlRdhEREdnO6jCora2Fj48PAMDHxwe1\ntbWd9vHz88Prr7+OESNGYPjw4Rg8eDCeeuop66u1ICeHXURERLbq9uE2er0eJpOp0+vJyckdfvby\n8oKXl1en/err65GdnY3y8nIMGjQIzz//PD799FO8+OKLFj8vKSmp/e86nQ46na7b4i9fBoqKgClT\nut2NiMhtGAwGGAwGux/X6nkGarUaBoMBvr6+qKmpQUxMDIqLizvsk5mZiS+//BIff/wxACAjIwN5\neXn44IMPOhdixVjZrVuBzz8HsrKs+Q2IiFyf4vMMEhISkJ6eDgBIT0/HnDlzOu0zcuRI5OXl4caN\nGxBC4ODBg9BoNNZX+yu8X0BEZB9Wtwzq6uowf/58VFRUICAgADt37sTgwYNhNBqxYsUK7N27F4Ds\n+tmxYwe8vb0xYcIEfPzxx+jfv3/nQnqZbq2twLBhwNmzgEplzW9AROT6PP5JZ8ePAytXAufOObAo\nIiInp3g3kdLy8oAnn1S6CiIi9+CyYXDqFDBpktJVEBG5B5cOg8hIpasgInIPLnnPoL4eGDECMJuB\nfv0cXBgRkRPz6HsGp08DEREMAiIie3HJMOD9AiIi+2IYEBERw4CIiFwwDEwm4No1YPRopSshInIf\nLhcG334rh5RaWCSViIis5HJhwC4iIiL7YxgQEZFrTToTQq5UeuYMYOenZxIRuSSPnHRWUQH0788g\nICKyN5cKA3YRERE5BsOAiIhcKwzOnwfCw5WugojI/bhUGBQWAnZ8hDIREf3CZUYTXbsGPPwwcPUq\nVyslImrjcaOJiouB4GAGARGRI7hMGLCLiIjIcVwmDIqKgJAQpasgInJPLhMGbBkQETmO1WGQmZmJ\n0NBQ9OvXD/n5+V3ul5ubC7VajaCgIKxbt87aj2PLgIjIgawOg7CwMGRlZSE6OrrLfVpbW/Hqq68i\nNzcXhYWF2LZtG4qKinr9WY2NcimKMWOsrZaIiLrjbe0b1Wp1j/ucPHkSY8aMQUBAAABg4cKF+OKL\nLxDSy6/4JSVAQABw991WFEpERD1y6D2D6upq+Pv7t/+sUqlQXV3d6+MUFfF+ARGRI3XbMtDr9TCZ\nTJ1eT0lJQXx8fI8H9+rl48iSkpLa/67T6aDT6QDIm8e8X0BEBBgMBhgMBrsft9swOHDggE0H9/Pz\nQ2VlZfvPlZWVUKlUXe5/exjcrqgIeOYZm0ohInILt39RBoC3337bLse1SzdRV1OhIyMjUVpaivLy\ncjQ1NWHHjh1ISEjo9fHZMiAiciyrwyArKwv+/v7Iy8tDXFwcYmNjAQBGoxFxcXEAAG9vb2zYsAEz\nZ86ERqPBggULen3zuKUFuHgRGDvW2kqJiKgnTr9QXUkJMGsW8P33ChRFROTkPGahOs48JiJyPKcP\nA848JiJyPKcPA7YMiIgcz+nDgBPOiIgcz6lvIAsB3H8/UF0NDBqkUGFERE7MI24gG43AwIEMAiIi\nR3PqMCgtBYKClK6CiMj9MQyIiIhhQEREDAMiIgLDgIiI4MRDS2/eBAYMAK5ckX8SEVFnbj+0tLIS\nGDKEQUBE1BecNgzYRURE1HcYBkRExDAgIiKGARERgWFARERw0qGlLS1ygbr6euDeexUujIjIibn1\n0NKKCmDYMAYBEVFfccowYBcREVHfYhgQERHDgIiIbAyDzMxMhIaGol+/fsjPz7e4T2VlJWJiYhAa\nGgqtVov169f3eNzSUiA42JbKiIioN2wKg7CwMGRlZSE6OrrLffr374/3338fFy5cQF5eHj744AMU\nFRV1e1y2DIiI+pa3LW9Wq9U97uPr6wtfX18AwMCBAxESEgKj0YiQkBCL+zc3y0XqRo+2pTIiIuqN\nPr1nUF5ejoKCAkRFRXWzDzB8OHD33X1XFxGRp+uxZaDX62EymTq9npKSgvj4+Dv+oIaGBsybNw9p\naWkYOHCgxX2SkpJQUgJ4eQEGgw46ne6Oj09E5AkMBgMMBoPdj2uXGcgxMTF47733MGHCBIv/3tzc\njNmzZyM2NharVq2yXMgvs+j+7/+A1lYgOdnWqoiI3J/TzUDuqhghBJYtWwaNRtNlENzu2DHg8cft\nVRUREd0Jm8IgKysL/v7+yMvLQ1xcHGJjYwEARqMRcXFxAIBvvvkGW7duxeHDhxEREYGIiAjk5uZa\nPF5LC3DyJPDYY7ZURUREveVUC9UVFAi88ALQw8hTIiL6hdN1E9kDu4iIiJThdGHwxBNKV0FE5Hmc\nKgy++YYtAyIiJTjVPYMhQwQuXwbucqqIIiJyXm55z2DyZAYBEZESnOrSyy4iIiJlMAyIiMi57hk0\nNAgMGKB0JURErsNe9wycKgycpBQiIpfhljeQiYhIGQwDIiJiGBAREcOAiIjAMCAiIjAMiIgIDAMi\nIgLDgIiIwDAgIiIwDIiICAwDIiICw4CIiMAwICIi2BAGmZmZCA0NRb9+/ZCfn9/tvq2trYiIiEB8\nfLy1H0dERA5kdRiEhYUhKysL0dHRPe6blpYGjUYDLy8vaz/OoxgMBqVLcBo8F7fwXNzCc2F/VoeB\nWq1GcHBwj/tVVVUhJycHy5cv5/MK7hD/Q7+F5+IWnotbeC7sz+H3DF577TW88847uItPuiciclre\n3f2jXq+HyWTq9HpKSsod9f/v2bMHw4YNQ0REBJOciMiZCRvpdDpx+vRpi//25z//WahUKhEQECB8\nfX3FfffdJxYvXmxx38DAQAGAGzdu3Lj1YgsMDLT1Mi6EEMLmZyDHxMTg3XffxcSJE7vd78iRI3j3\n3Xexe/duWz6OiIgcwOqO/KysLPj7+yMvLw9xcXGIjY0FABiNRsTFxVl8D0cTERE5J5tbBkRE5PoU\nH+KTm5sLtVqNoKAgrFu3TulyHK6yshIxMTEIDQ2FVqvF+vXrAQB1dXXQ6/UIDg7GjBkzYDab29+z\ndu1aBAUFQa1WY//+/UqV7jC/npToqefCbDZj3rx5CAkJgUajwYkTJzz2XLz//vvQarUICwvDokWL\n0NjY6DHn4uWXX4aPjw/CwsLaX7Pmdz99+jTCwsIQFBSEP/3pTz1/sF3uPFippaVFBAYGirKyMtHU\n1CTCw8NFYWGhkiU5XE1NjSgoKBBCCHH16lURHBwsCgsLxRtvvCHWrVsnhBAiNTVVrFmzRgghxIUL\nF0R4eLhoamoSZWVlIjAwULS2tipWvyO89957YtGiRSI+Pl4IITz2XCxZskR88sknQgghmpubhdls\n9shzUVVVJUaNGiV+/vlnIYQQ8+fPF5s3b/aYc/HVV1+J/Px8odVq21/rze9+8+ZNIYQQkyZNEidO\nnBBCCBEbGyv27dvX7ecqGgbHjh0TM2fObP957dq1Yu3atQpW1PeeeeYZceDAATF27FhhMpmEEDIw\nxo4dK4QQIiUlRaSmprbvP3PmTHH8+HFFanWEyspKMX36dHHo0CExe/ZsIYTwyHNhNpvFqFGjOr3u\nieeiqqpK+Pv7i7q6OtHc3Cxmz54t9u/f71HnoqysrEMY9PZ3NxqNQq1Wt7++bds2sXLlym4/U9Fu\nourqavj7+7f/rFKpUF1drWBFfau8vBwFBQWIiopCbW0tfHx8AAA+Pj6ora0FIG/Iq1Sq9ve42zmy\nNCnRE89FWVkZHn74YSxduhQTJkzAihUrcO3aNY88F35+fnj99dcxYsQIDB8+HIMHD4Zer/fIc9Gm\nt7/7r1/38/Pr8ZwoGgaePLqooaEBc+fORVpaGu6///4O/+bl5dXtuXGX83b7pETRxTgGTzkXLS0t\nyM/Px+9//3vk5+djwIABSE1N7bCPp5yL+vp6ZGdno7y8HEajEQ0NDdi6dWuHfTzlXFjS0+9uLUXD\nwM/PD5WVle0/V1ZWdkgzd9Xc3Iy5c+di8eLFmDNnDgCZ9m2zvWtqajBs2DAAnc9RVVUV/Pz8+r5o\nBzh27Biys7MxatQovPDCCzh06BAWL17skedCpVJBpVJh0qRJAIB58+YhPz8fvr6+HncuDh48iFGj\nRmHo0KHw9vbGc889h+PHj3vkuWjTm/8nVCoV/Pz8UFVV1eH1ns6JomEQGRmJ0tJSlJeXo6mpCTt2\n7EBCQoKSJTmcEALLli2DRqPBqlWr2l9PSEhAeno6ACA9Pb09JBISErB9+3Y0NTWhrKwMpaWlePTR\nRxWp3d5SUlJQWVmJsrIybN++HdOmTUNGRoZHngtfX1/4+/ujpKQEgLwghoaGIj4+3uPOxciRI5GX\nl4cbN25ACIGDBw9Co9F45Llo09v/J3x9ffHAAw/gxIkTEEIgIyOj/T1dstcND2vl5OSI4OBgERgY\nKFJSUpQux+G+/vpr4eXlJcLDw8X48ePF+PHjxb59+8QPP/wgpk+fLoKCgoRerxf19fXt70lOThaB\ngYFi7NixIjc3V8HqHcdgMLSPJvLUc3HmzBkRGRkpxo0bJ5599llhNps99ly89dZbQq1WC61WK5Ys\nWSKampo85lwsXLhQPPLII6J///5CpVKJjRs3WvW7f/vtt0Kr1YrAwEDxhz/8ocfP5aQzIiJSftIZ\nEREpj2H80YQ7AAAAJklEQVRAREQMAyIiYhgQEREYBkREBIYBERGBYUBERGAYEBERgP8HnDGFQoWx\n3O4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x31ec528>"
       ]
      }
     ],
     "prompt_number": 312
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 301
    }
   ],
   "metadata": {}
  }
 ]
}