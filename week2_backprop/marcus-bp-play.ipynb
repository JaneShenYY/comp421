{
 "metadata": {
  "name": "marcus-bp-play",
  "signature": "sha256:cfe894d951f9f30e6b6956d36a0920a87647b95c772379272ecd21833937fdf6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "It's NOT working yet - see the graph at the end, which should go UP UP UP!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%matplotlib` not found."
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    return x\n",
      "\n",
      "def dfunk( phi ):\n",
      "    # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    x = funk(phi)\n",
      "    dx = x*(1-x)\n",
      "    return dx\n",
      "\n",
      "def dfunk_from_funk( x ):\n",
      "    # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    dx = x*(1-x)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 10\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.02  0.54 -0.71 -0.87]\n",
        " [ 1.31 -0.06 -0.85 -0.75]\n",
        " [-0.9  -0.27  0.94  0.67]]\n",
        "[[0 1]\n",
        " [0 1]\n",
        " [1 0]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood = error # e.g. if output is too low, this should be positive.\n",
      "    return Good, dGood"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "#architecture = [inpats.shape[1], 3, 5, targets.shape[1]]\n",
      "architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('There are this many neurons in each layer: ', [4, 2])\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] # going to be a list, one elt per layer, each elt being a vector\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('layer 0 activations have shape ', (10, 4))\n",
        "('layer 1 activations have shape ', (10, 2))\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# there are a bunch of handy things of the same size as X too. Who cares what's in them: we will overwrite anyway.\n",
      "dX, epsilon = [], []\n",
      "for L in range(len(architecture)): \n",
      "    dX.append(np.copy(X[L]))\n",
      "    epsilon.append(np.copy(X[L]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    init_tmp_w = rng.normal(0,1,size=(Nouts, Nins))\n",
      "    W.append(init_tmp_w)\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('layer 0 weights have shape ', ())\n",
        "('layer 1 weights have shape ', (2, 4))\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X,W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X,W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test outer product - this version really sucks as it has a loop over patterns. Must Try Harder.\n",
      "def inefficient_outer_product(x_lowLayer, x_highLayer):\n",
      "    npats = x_lowLayer.shape[0]\n",
      "    n_lowLayer  = x_lowLayer.shape[1]\n",
      "    n_highLayer = x_highLayer.shape[1]\n",
      "    #print ('xlow and xhi shapes: ',x_lowLayer.shape, x_highLayer.shape)\n",
      "    ans = np.zeros((npats, n_highLayer, n_lowLayer))\n",
      "    for i in range(npats):\n",
      "        #print ('shapes: ', ans[i,:,:].shape, np.multiply.outer(x_highLayer[i], x_lowLayer[i]).shape)\n",
      "        ans[i,:,:] = np.multiply.outer(x_highLayer[i], x_lowLayer[i])\n",
      "    return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good, dgood = goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        #print ('layer ',L)\n",
      "        psi = epsilon * dfunk_from_funk(X[L])\n",
      "        #print (psi.shape)\n",
      "        dW[L] = inefficient_outer_product(X[L-1], X[L]).sum(0)\n",
      "        epsilon = np.dot(psi, W[L])\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(X, W, targets):\n",
      "    # check the gradient directly via perturbations to every weight.\n",
      "    # Practically this is daft, but I need it for debugging.\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    dW, prev_change = [np.array(None)], [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        dW.append(np.copy(W[L]))\n",
      "        prev_change.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "    return W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals = []\n",
      "for t in range(100):\n",
      "    W = learn(X, W, targets, learning_rate=0.001, momentum=0.0, num_steps=100)\n",
      "\n",
      "    good, dgood = goodness(X[-1], targets)\n",
      "    vals.append(good.sum())\n",
      "\n",
      "plt.plot(vals)\n",
      "print(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array(None, dtype=object), array([[-28.14,  21.66,  16.67, -32.63],\n",
        "       [-22.58, -42.39, -31.83, -56.27]])]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xb12d5a6c>"
       ]
      }
     ],
     "prompt_number": 240
    }
   ],
   "metadata": {}
  }
 ]
}