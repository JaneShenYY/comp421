{
 "metadata": {
  "name": "",
  "signature": "sha256:c01426c1282903294d1a7a580880db48ba53a5dab12aa31747dc3055f0908058"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "Note the use of \"checkgrad\" - a very common, useful, thing to have.\n",
      "\n",
      "It \"works\" (by some measure).\n",
      "Issues:\n",
      "  * the neural net has no biases yet\n",
      "  * my \"outer product\" has an evil loop over training patterns in it: ugly and slow!\n",
      "  * the learning problem is just random - better if we could read in a training set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# THESE FUNKS MUST MATCH ONE ANOTHER................\n",
      "\n",
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    \n",
      "    #ALT: rectified linear goes like this\n",
      "    #x = phi * (phi>0.0)\n",
      "    return x\n",
      "\n",
      "\"\"\"\n",
      "def dfunk( phi ):\n",
      "    # this is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    x = funk(phi)\n",
      "    dx = x*(1-x)\n",
      "    return dx\n",
      "\"\"\"\n",
      "\n",
      "def dfunk_from_funk( x ):  # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    \n",
      "    # This is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    # But the input argument isn't phi here - it's the function value itself.\n",
      "    \n",
      "    dx = x*(1-x)\n",
      "    #ALT: rectified linear goes like this\n",
      "    #dx = 1.0*(x>0.0)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 50\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 2.051  0.758  0.282  0.584]\n",
        " [ 0.816  0.826 -1.266 -1.468]\n",
        " [ 0.139 -0.354 -0.22   0.683]]\n",
        "[[1 1]\n",
        " [0 1]\n",
        " [1 0]]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
      "    return Good_vec.sum(), Good_vec, dGood_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "architecture = [inpats.shape[1], 3, targets.shape[1]]\n",
      "#architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are this many neurons in each layer:  [4, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] \n",
      "# X is going to be a list giving the activations of successive layers. \n",
      "# Each one is a matrix, whose columns are the neurons in that layer.\n",
      "# Each row in the matrix corresponds to a training item.\n",
      "# So all the matrices in the list X will have the same number of rows.\n",
      "\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 activations have shape  (50, 4)\n",
        "layer 1 activations have shape  (50, 3)\n",
        "layer 2 activations have shape  (50, 2)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set up the weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W  = [np.array(None)]\n",
      "dW = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
      "\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
      "    dW.append(0.0 * np.copy(W[L]))\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 weights have shape  ()\n",
        "layer 1 weights have shape  (3, 4)\n",
        "layer 2 weights have shape  (2, 3)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X, W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test outer product - this version really sucks as it has a loop over patterns. Must Try Harder.\n",
      "def inefficient_outer_product(x_lowLayer, x_highLayer):\n",
      "    npats = x_lowLayer.shape[0]\n",
      "    n_lowLayer  = x_lowLayer.shape[1]\n",
      "    n_highLayer = x_highLayer.shape[1]\n",
      "    #print ('xlow and xhi shapes: ',x_lowLayer.shape, x_highLayer.shape)\n",
      "    ans = np.zeros((npats, n_highLayer, n_lowLayer))\n",
      "    for i in range(npats):\n",
      "        #print ('shapes: ', ans[i,:,:].shape, np.multiply.outer(x_highLayer[i], x_lowLayer[i]).shape)\n",
      "        ans[i,:,:] = np.outer(x_highLayer[i,:], x_lowLayer[i,:])\n",
      "    i=0\n",
      "    #print ('highlayer activity: ', x_highLayer[i,:])\n",
      "    #print ('lowlayer activity: ', x_lowLayer[i,:])\n",
      "    #print ('outer product: ',np.outer(x_highLayer[i,:], x_lowLayer[i,:]))\n",
      "    return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    npats = X[0].shape[0]\n",
      "    #print ('an output: ', X[-1][0])\n",
      "    #print ('a target: ', targets[0])\n",
      "    #print ('an epsilon: ', epsilon[0])\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        #print ('layer ',L)\n",
      "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
      "        \"\"\"\n",
      "        n1, n2 = X[L-1].shape[1], psi.shape[1]\n",
      "        A = np.tile(X[L-1],n2).reshape(npats,n2,n1)\n",
      "        B = np.repeat(psi,n1).reshape(npats,n2,n1)\n",
      "        dw_per_pat = (A*B).sum(0)\n",
      "        \"\"\"\n",
      "        dw_per_pat = inefficient_outer_product(X[L-1], psi)\n",
      "        #print ('EG:\\t in_act=%.3f,\\n\\t out_act=%.3f, \\n\\t epsilon=%.3f,' %(X[L-1][0,0], X[L][0,0], epsilon[0]))\n",
      "        #print ('\\t dXdphi=%.3f, \\n\\t dW=%.3f' %(dfunk_from_funk(X[L])[0], dw_per_pat[0,0,0]))\n",
      "        dW[L] = dw_per_pat.sum(0) # outer product multiply\n",
      "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X, W)\n",
      "dW = backward_pass(X, W, dW, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(dW, X, W, targets):\n",
      "    # Check the gradient directly, via perturbations to every weight.\n",
      "    # This is completely daft in practical terms, but very useful for debugging.\n",
      "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
      "    tiny = 0.0001\n",
      "    \n",
      "    dW_test = [np.array(None)]\n",
      "    for L in range(1,len(W)):\n",
      "        dW_test.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    X = forward_pass(X,W)\n",
      "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "    \n",
      "    for L in range(1,len(X)):\n",
      "        for j in range(W[L].shape[0]): # index of destination node\n",
      "            for i in range(W[L].shape[1]): # index of origin node\n",
      "                # perturb that weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
      "                # compute and store the empirical gradient estimate\n",
      "                X = forward_pass(X,W)\n",
      "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny\n",
      "                \n",
      "                # unperturb the weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
      "                \n",
      "    # show the result?\n",
      "    for L in range(1,len(X)):\n",
      "        print ('-------------- layer %d --------------' %(L))\n",
      "        #print ('acts:', X[L])\n",
      "        print ('calculated gradients:')\n",
      "        print (dW[L])\n",
      "        print ('empirical gradients:')\n",
      "        print (dW_test[L])\n",
      "\n",
      "checkgrad(dW, X, W, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------- layer 1 --------------\n",
        "calculated gradients:\n",
        "[-0.006  0.001 -0.002  0.016]\n",
        "empirical gradients:\n",
        "[[ 0.005  0.011 -0.004  0.005]\n",
        " [-0.006 -0.015  0.003 -0.001]\n",
        " [-0.006  0.006 -0.001  0.012]]\n",
        "-------------- layer 2 --------------\n",
        "calculated gradients:\n",
        "[ 0.01  -0.022  0.031]\n",
        "empirical gradients:\n",
        "[[-0.005 -0.007  0.025]\n",
        " [ 0.014 -0.015  0.005]]\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## yay.\n",
      "The gradient seems to be right for the full MLP, so that's... progress!\n",
      "\n",
      "Let's try learning the problem then...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    times, vals = [], []\n",
      "    next_time = 0\n",
      "    \n",
      "    prev_change = [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        prev_change.append(0.0 * np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        \n",
      "        # this is just record-keeping.......\n",
      "        if step == next_time:\n",
      "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "            vals.append(good_sum)\n",
      "            times.append(step)\n",
      "            next_time = step + 10\n",
      "\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "\n",
      "    return W, times, vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.8, num_steps=1000)\n",
      "plt.plot(vals, times)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[<matplotlib.lines.Line2D at 0x4079c68>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkiIiEidFBIiIlInhYSIiNTp/wGwnYTXMqBbewAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3fd1e08>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npats=4\n",
      "n1 = 3\n",
      "testX   = np.arange(npats*n1).reshape(npats,n1)\n",
      "n2 = 2\n",
      "testpsi = np.arange(npats*n2).reshape(npats,n2)\n",
      "#print('testX:\\n',testX )\n",
      "#print('testpsi:\\n',testpsi , '\\n\\n')\n",
      "print('inefficient:\\n', inefficient_outer_product(testX, testpsi).sum(0) )\n",
      "\n",
      "n1, n2 = testX.shape[1], testpsi.shape[1]\n",
      "A = np.tile(testX,n2).reshape(npats,n2,n1)\n",
      "B = np.repeat(testpsi,n1).reshape(npats,n2,n1)\n",
      "ans = (A*B).sum(0)\n",
      "print (ans)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "inefficient:\n",
        " [[  84.   96.  108.]\n",
        " [ 102.  118.  134.]]\n",
        "[[ 84  96 108]\n",
        " [102 118 134]]\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}