{
 "metadata": {
  "name": "",
  "signature": "sha256:d37e3c8d317a413f7058386eacb14e0f550b4d1228bc6ee10e57ea2f75de00b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# horsing around with the backprop algorithm\n",
      "Marcus started this see how quickly he could get backprop to stand up.\n",
      "\n",
      "Note the use of \"checkgrad\" - a very common, useful, thing to have.\n",
      "\n",
      "It \"works\" (by some measure).\n",
      "Issues:\n",
      "  * the neural net has no biases yet\n",
      "  * my \"outer product\" has an evil loop over training patterns in it: ugly and slow!\n",
      "  * the learning problem is just random - better if we could read in a training set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy.random as rng\n",
      "np.set_printoptions(precision = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 372
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### specify a neuron transfer function ('funk'), and its derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# THESE FUNKS MUST MATCH ONE ANOTHER................\n",
      "\n",
      "def funk( phi ):  \n",
      "    # phi is always going to be a weighted sum (probably a matrix of).\n",
      "    x = 1.0/ (1.0 + np.exp(-phi))\n",
      "    \n",
      "    #ALT: rectified linear goes like this\n",
      "    #x = phi * (phi>0.0)\n",
      "    return x\n",
      "\n",
      "\"\"\"\n",
      "def dfunk( phi ):\n",
      "    # this is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    x = funk(phi)\n",
      "    dx = x*(1-x)\n",
      "    return dx\n",
      "\"\"\"\n",
      "\n",
      "def dfunk_from_funk( x ):  # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
      "    \n",
      "    # This is the gradient of the transfer function (funk)\n",
      "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
      "    # But the input argument isn't phi here - it's the function value itself.\n",
      "    \n",
      "    dx = x*(1-x)\n",
      "    #ALT: rectified linear goes like this\n",
      "    #dx = 1.0*(x>0.0)\n",
      "    return dx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 373
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### get or make some training data\n",
      "Got to have something to work on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
      "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
      "Nins, Nouts, Npats = 4, 2, 50\n",
      "tmp_weights = rng.normal(0,1,size=(Nins,Nouts))\n",
      "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
      "phi = np.dot(inpats, tmp_weights)\n",
      "targets = 1*(phi >= 0.0)\n",
      "print (inpats[:3, :])\n",
      "print (targets[:3, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.002  0.345  0.659  0.506]\n",
        " [-2.378 -0.354 -0.007  0.587]\n",
        " [ 0.539 -1.567 -2.05  -0.15 ]]\n",
        "[[1 1]\n",
        " [1 0]\n",
        " [0 1]]\n"
       ]
      }
     ],
     "prompt_number": 375
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The function we're climbing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_goodness(outputs, targets):\n",
      "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
      "    error = targets - outputs\n",
      "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
      "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
      "    return Good_vec.sum(), Good_vec, dGood_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 376
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set the network's architecture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Npats = inpats.shape[0]\n",
      "architecture = [inpats.shape[1], 3, targets.shape[1]]\n",
      "#architecture = [inpats.shape[1], targets.shape[1]]\n",
      "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are this many neurons in each layer:  [4, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 377
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [inpats] \n",
      "# X is going to be a list giving the activations of successive layers. \n",
      "# Each one is a matrix, whose columns are the neurons in that layer.\n",
      "# Each row in the matrix corresponds to a training item.\n",
      "# So all the matrices in the list X will have the same number of rows.\n",
      "\n",
      "for L in range(1, len(architecture)):\n",
      "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
      "\n",
      "for L in range(len(architecture)): \n",
      "    print('layer %d activations have shape ' %(L), X[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 activations have shape  (50, 4)\n",
        "layer 1 activations have shape  (50, 3)\n",
        "layer 2 activations have shape  (50, 2)\n"
       ]
      }
     ],
     "prompt_number": 378
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### set up the weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
      "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
      "W  = [np.array(None)]\n",
      "dW = [np.array(None)]\n",
      "for L in range(1,len(X)):\n",
      "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
      "\n",
      "    Nins = X[L-1].shape[1]\n",
      "    Nouts = X[L].shape[1]\n",
      "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
      "    dW.append(0.0 * np.copy(W[L]))\n",
      "\n",
      "for L in range(len(W)):\n",
      "    print('layer %d weights have shape ' %(L), W[L].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "layer 0 weights have shape  ()\n",
        "layer 1 weights have shape  (3, 4)\n",
        "layer 2 weights have shape  (2, 3)\n"
       ]
      }
     ],
     "prompt_number": 379
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### forward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward_pass(X, W):\n",
      "    for L in range(1,len(X)):\n",
      "        x = X[L-1].transpose()\n",
      "        # print (L, W[L].shape, x.shape)\n",
      "        X[L] = funk(np.dot(W[L], x).transpose())\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 380
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### backward pass"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test outer product - this version really sucks as it has a loop over patterns. Must Try Harder.\n",
      "def inefficient_outer_product(x_lowLayer, x_highLayer):\n",
      "    npats = x_lowLayer.shape[0]\n",
      "    n_lowLayer  = x_lowLayer.shape[1]\n",
      "    n_highLayer = x_highLayer.shape[1]\n",
      "    #print ('xlow and xhi shapes: ',x_lowLayer.shape, x_highLayer.shape)\n",
      "    ans = np.zeros((npats, n_highLayer, n_lowLayer))\n",
      "    for i in range(npats):\n",
      "        #print ('shapes: ', ans[i,:,:].shape, np.multiply.outer(x_highLayer[i], x_lowLayer[i]).shape)\n",
      "        ans[i,:,:] = np.outer(x_highLayer[i,:], x_lowLayer[i,:])\n",
      "    i=0\n",
      "    #print ('highlayer activity: ', x_highLayer[i,:])\n",
      "    #print ('lowlayer activity: ', x_lowLayer[i,:])\n",
      "    #print ('outer product: ',np.outer(x_highLayer[i,:], x_lowLayer[i,:]))\n",
      "    return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 381
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def backward_pass(X, W, dW, targets):\n",
      "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "    epsilon = dgood\n",
      "    #print ('an output: ', X[-1][0])\n",
      "    #print ('a target: ', targets[0])\n",
      "    #print ('an epsilon: ', epsilon[0])\n",
      "    for L in range(len(X)-1,0,-1):\n",
      "        #print ('layer ',L)\n",
      "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
      "        dw_per_pat = inefficient_outer_product(X[L-1], psi)\n",
      "        #print ('EG:\\t in_act=%.3f,\\n\\t out_act=%.3f, \\n\\t epsilon=%.3f,' %(X[L-1][0,0], X[L][0,0], epsilon[0]))\n",
      "        #print ('\\t dXdphi=%.3f, \\n\\t dW=%.3f' %(dfunk_from_funk(X[L])[0], dw_per_pat[0,0,0]))\n",
      "        dW[L] = dw_per_pat.sum(0) # outer product multiply\n",
      "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
      "    return dW"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 382
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = forward_pass(X, W)\n",
      "dW = backward_pass(X, W, dW, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 383
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkgrad(dW, X, W, targets):\n",
      "    # Check the gradient directly, via perturbations to every weight.\n",
      "    # This is completely daft in practical terms, but very useful for debugging.\n",
      "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
      "    tiny = 0.0001\n",
      "    \n",
      "    dW_test = [np.array(None)]\n",
      "    for L in range(1,len(W)):\n",
      "        dW_test.append(0.0*np.copy(W[L]))\n",
      "    \n",
      "    X = forward_pass(X,W)\n",
      "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "    \n",
      "    for L in range(1,len(X)):\n",
      "        for j in range(W[L].shape[0]): # index of destination node\n",
      "            for i in range(W[L].shape[1]): # index of origin node\n",
      "                # perturb that weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
      "                # compute and store the empirical gradient estimate\n",
      "                X = forward_pass(X,W)\n",
      "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
      "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny\n",
      "                \n",
      "                # unperturb the weight\n",
      "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
      "                \n",
      "    # show the result?\n",
      "    for L in range(1,len(X)):\n",
      "        print ('-------------- layer %d --------------' %(L))\n",
      "        #print ('acts:', X[L])\n",
      "        print ('calculated gradients:')\n",
      "        print (dW[L])\n",
      "        print ('empirical gradients:')\n",
      "        print (dW_test[L])\n",
      "\n",
      "checkgrad(dW, X, W, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------- layer 1 --------------\n",
        "calculated gradients:\n",
        "[[ 0.155 -0.089 -0.268  0.095]\n",
        " [-0.06   0.043  0.128 -0.037]\n",
        " [-0.036  0.025  0.075 -0.022]]\n",
        "empirical gradients:\n",
        "[[ 0.155 -0.089 -0.268  0.095]\n",
        " [-0.06   0.043  0.128 -0.037]\n",
        " [-0.036  0.025  0.075 -0.022]]\n",
        "-------------- layer 2 --------------\n",
        "calculated gradients:\n",
        "[[ 0.158 -0.116 -0.011]\n",
        " [ 0.57   0.836  0.624]]\n",
        "empirical gradients:\n",
        "[[ 0.158 -0.116 -0.011]\n",
        " [ 0.57   0.835  0.624]]\n"
       ]
      }
     ],
     "prompt_number": 384
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## yay.\n",
      "The gradient seems to be right for the full MLP, so that's... progress!\n",
      "\n",
      "Let's try learning the problem then...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
      "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
      "    times, vals = [], []\n",
      "    next_time = 0\n",
      "    \n",
      "    prev_change = [np.array(None)]\n",
      "    for L in range(1,len(X)):\n",
      "        prev_change.append(0.0 * np.copy(W[L]))\n",
      "    \n",
      "    # now for the learning iterations\n",
      "    for step in range(num_steps):\n",
      "        X = forward_pass(X,W)\n",
      "        \n",
      "        # this is just record-keeping.......\n",
      "        if step == next_time:\n",
      "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
      "            vals.append(good_sum)\n",
      "            times.append(step)\n",
      "            next_time = step + 10\n",
      "\n",
      "        dW = backward_pass(X, W, dW, targets)\n",
      "        for L in range(1,len(X)):\n",
      "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
      "            W[L] = W[L] + change\n",
      "            prev_change[L] = change\n",
      "\n",
      "\n",
      "    return W, times, vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 385
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.8, num_steps=1000)\n",
      "plt.plot(vals, times)\n",
      "print(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array(None, dtype=object), array([[ 0.411, -1.444, -3.797, -1.092],\n",
        "       [ 5.278, -1.232, -2.087,  2.913],\n",
        "       [-1.017,  1.925,  4.043,  0.296]]), array([[-4.741, -0.016,  5.458],\n",
        "       [-2.925,  6.875, -3.746]])]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x4201038>"
       ]
      }
     ],
     "prompt_number": 386
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 301
    }
   ],
   "metadata": {}
  }
 ]
}