{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horsing around with the backprop algorithm\n",
    "Marcus started this see how quickly he could get backprop to stand up.\n",
    "\n",
    "Note the use of \"checkgrad\", which exhaustively confirms that the gradient calculation is in fact correct - not something to run all the time but a useful check to have.\n",
    "\n",
    "Issues:\n",
    "  * the neural net has no biases yet\n",
    "  * the learning problem is just random - better if we could read in a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "np.set_printoptions(precision = 2, suppress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify a neuron transfer function ('funk'), and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THESE FUNKERS MUST MATCH ONE ANOTHER................\n",
    "\n",
    "def funk( phi ):  \n",
    "    # phi is always going to be a weighted sum (probably a matrix of).\n",
    "    x = 1.0/ (1.0 + np.exp(-phi))\n",
    "    \n",
    "    #ALT: rectified linear goes like this\n",
    "    #x = phi * (phi>0.0)\n",
    "    return x\n",
    "\n",
    "def dfunk_from_funk( x ):  # MUST MATCH WHAT YOU PUT HERE with the funk function.\n",
    "    # This is the gradient of the transfer function (funk)\n",
    "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
    "    # But the input argument isn't phi here - it's the function value itself.\n",
    "\n",
    "    dx = x*(1-x)\n",
    "    #ALT: rectified linear goes like this\n",
    "    #dx = 1.0*(x>0.0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get or make some training data\n",
    "Got to have something to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64  0.22 -1.35 -2.32 -0.31  0.99 -0.23 -1.15]\n",
      " [ 0.52 -0.03 -1.01 -1.16  0.12  0.21  0.38 -0.09]\n",
      " [-0.22  1.6   1.15 -0.77  0.44  0.73 -0.41 -0.97]]\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
    "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
    "Nins, Nouts, Npats = 8, 2, 100\n",
    "inpats = rng.normal(0,1,size=(Npats,Nins))\n",
    "tmpNhids = 10\n",
    "tmp_weights = rng.normal(0,1,size=(Nins,tmpNhids))\n",
    "hidphi = np.dot(inpats, tmp_weights)\n",
    "tmp_weights = rng.normal(0,1,size=(tmpNhids,Nouts))\n",
    "phi = np.dot(funk(hidphi), tmp_weights)\n",
    "targets = 1.0* (funk(phi) > 0.5)\n",
    "print (inpats[:3, :])\n",
    "print (targets[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function we're climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_goodness(outputs, targets):\n",
    "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
    "    error = targets - outputs\n",
    "    Good_vec = -0.5*np.power(error,2.0) # inverted parabola centered on the target outputs\n",
    "    dGood_vec = error # e.g. if output is too low, this should be positive.\n",
    "    return Good_vec.sum(), Good_vec, dGood_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the network's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many neurons in each layer:  [8, 5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "Npats = inpats.shape[0]\n",
    "architecture = [inpats.shape[1], 5, 3, targets.shape[1]]\n",
    "#architecture = [inpats.shape[1], targets.shape[1]]\n",
    "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 activations have shape  (100, 8)\n",
      "layer 1 activations have shape  (100, 5)\n",
      "layer 2 activations have shape  (100, 3)\n",
      "layer 3 activations have shape  (100, 2)\n"
     ]
    }
   ],
   "source": [
    "X = [inpats] \n",
    "# X is going to be a list giving the activations of successive layers. \n",
    "# Each one is a matrix, whose columns are the neurons in that layer.\n",
    "# Each row in the matrix corresponds to a training item.\n",
    "# So all the matrices in the list X will have the same number of rows.\n",
    "\n",
    "for L in range(1, len(architecture)):\n",
    "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
    "\n",
    "for L in range(len(architecture)): \n",
    "    print('layer %d activations have shape ' %(L), X[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 weights have shape  ()\n",
      "layer 1 weights have shape  (5, 8)\n",
      "layer 2 weights have shape  (3, 5)\n",
      "layer 3 weights have shape  (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
    "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
    "# NOTE: no implementation of bias weights here, yet!\n",
    "W  = [np.array(None)]\n",
    "dW = [np.array(None)]\n",
    "for L in range(1,len(X)):\n",
    "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
    "\n",
    "    Nins = X[L-1].shape[1]\n",
    "    Nouts = X[L].shape[1]\n",
    "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
    "    dW.append(0.0 * np.copy(W[L]))\n",
    "\n",
    "for L in range(len(W)):\n",
    "    print('layer %d weights have shape ' %(L), W[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W):\n",
    "    for L in range(1,len(X)):\n",
    "        x = X[L-1].transpose()\n",
    "        # print (L, W[L].shape, x.shape)\n",
    "        X[L] = funk(np.dot(W[L], x).transpose())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(X, W, dW, targets):\n",
    "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "    epsilon = dgood\n",
    "    npats = X[0].shape[0]\n",
    "    for L in range(len(X)-1,0,-1):\n",
    "        psi = epsilon * dfunk_from_funk(X[L]) # elt-wise multiply\n",
    "        n1, n2 = X[L-1].shape[1], psi.shape[1]\n",
    "        A = np.tile(X[L-1],n2).reshape(npats,n2,n1)\n",
    "        B = np.repeat(psi,n1).reshape(npats,n2,n1)        \n",
    "        dW[L] = (A*B).sum(0) # outer product multiply\n",
    "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = forward_pass(X, W)\n",
    "dW = backward_pass(X, W, dW, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkgrad(dW, X, W, targets):\n",
    "    # Check the gradient directly, via perturbations to every weight.\n",
    "    # This is completely daft in practical terms, but very useful for debugging.\n",
    "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
    "    tiny = 0.0001\n",
    "    \n",
    "    dW_test = [np.array(None)]\n",
    "    for L in range(1,len(W)):\n",
    "        dW_test.append(0.0*np.copy(W[L]))\n",
    "    \n",
    "    X = forward_pass(X,W)\n",
    "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "    \n",
    "    for L in range(1,len(X)):\n",
    "        for j in range(W[L].shape[0]): # index of destination node\n",
    "            for i in range(W[L].shape[1]): # index of origin node\n",
    "                # perturb that weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
    "                # compute and store the empirical gradient estimate\n",
    "                X = forward_pass(X,W)\n",
    "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny                \n",
    "                # unperturb the weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
    "                \n",
    "    # show the result?\n",
    "    for L in range(1,len(X)):\n",
    "        print ('-------------- layer %d --------------' %(L))\n",
    "        print ('calculated gradients:')\n",
    "        print (dW[L])\n",
    "        print ('empirical gradients:')\n",
    "        print (dW_test[L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- layer 1 --------------\n",
      "calculated gradients:\n",
      "[[ 0.    0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n",
      " [-0.    0.   -0.   -0.    0.   -0.    0.    0.  ]\n",
      " [-0.   -0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [-0.    0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n",
      " [-0.    0.   -0.   -0.    0.   -0.01  0.    0.  ]]\n",
      "empirical gradients:\n",
      "[[ 0.    0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n",
      " [-0.    0.   -0.   -0.    0.   -0.    0.    0.  ]\n",
      " [-0.   -0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [-0.    0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n",
      " [-0.    0.   -0.   -0.    0.   -0.01  0.    0.  ]]\n",
      "-------------- layer 2 --------------\n",
      "calculated gradients:\n",
      "[[-0.03 -0.04 -0.03 -0.04 -0.04]\n",
      " [ 0.05  0.05  0.06  0.05  0.05]\n",
      " [ 0.08  0.08  0.08  0.08  0.08]]\n",
      "empirical gradients:\n",
      "[[-0.03 -0.04 -0.03 -0.04 -0.04]\n",
      " [ 0.05  0.05  0.06  0.05  0.05]\n",
      " [ 0.08  0.08  0.08  0.08  0.08]]\n",
      "-------------- layer 3 --------------\n",
      "calculated gradients:\n",
      "[[ 2.44  2.72  2.69]\n",
      " [ 3.41  3.82  3.76]]\n",
      "empirical gradients:\n",
      "[[ 2.44  2.72  2.69]\n",
      " [ 3.41  3.82  3.76]]\n"
     ]
    }
   ],
   "source": [
    "checkgrad(dW, X, W, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yay.\n",
    "The gradient seems to be right for the full MLP, so that's... progress!\n",
    "\n",
    "Let's try learning the problem then...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
    "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
    "    times, vals = [], []\n",
    "    next_time = 0\n",
    "    \n",
    "    prev_change = [np.array(None)]\n",
    "    for L in range(1,len(X)):\n",
    "        prev_change.append(0.0 * np.copy(W[L]))\n",
    "    \n",
    "    # now for the learning iterations\n",
    "    for step in range(num_steps):\n",
    "        X = forward_pass(X,W)\n",
    "        \n",
    "        # this is just record-keeping.......\n",
    "        if step == next_time:\n",
    "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "            vals.append(good_sum)\n",
    "            times.append(step)\n",
    "            next_time = step + 10\n",
    "\n",
    "        dW = backward_pass(X, W, dW, targets)\n",
    "        for L in range(1,len(X)):\n",
    "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
    "            W[L] = W[L] + change\n",
    "            prev_change[L] = change\n",
    "\n",
    "\n",
    "    return W, times, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3069bf8>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2fb0778>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, vals, times = learn(X, W, dW, targets, learning_rate=0.01, momentum=0.5, num_steps=10000)\n",
    "plt.plot(vals, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
